{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparable-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pipenv run pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-victor",
   "metadata": {},
   "source": [
    "### Using Spark SQL in Spark Applications\n",
    "#### Using Spark SQL in Spark Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precious-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "after-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "veterinary-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/departuredelays.csv\"\n",
    "\n",
    "file_schema = \"date STRING, delay INT, distance INT,origin STRING, destination STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "great-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By setting inferSchema=true, \n",
    "#Spark will automatically go through the csv file\n",
    "#and infer the schema of each column.\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(file_schema)\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .load(csv_file)\n",
    "    \n",
    ")\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "desperate-hands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nschema = \"`date` STRING, `delay` INT, `distance` INT,\\n`origin` STRING, `destination` STRING\"\\n\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you want to specify a schema, you can use a DDL-formatted\n",
    "#string. For example:\n",
    "\n",
    "# In Python\n",
    "'''\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT,\n",
    "`origin` STRING, `destination` STRING\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "occasional-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "honey-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "spark.sql('''\n",
    "SELECT distance, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE distance > 1000 ORDER BY distance DESC\n",
    "''').show(10)\n",
    "\n",
    "# With DateFrame API\n",
    "(\n",
    "    df.select('distance', 'origin', 'destination')\n",
    "    .where('distance > 1000')\n",
    "    .orderBy('distance', ascending=False)\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "planned-march",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "\n",
    "spark.sql('''\n",
    "\n",
    "SELECT date, delay, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' and destination = 'ORD'\n",
    "ORDER BY delay DESC\n",
    "''').show(10)\n",
    "\n",
    "# With API\n",
    "\n",
    "cond1 = col(\"delay\") > 120\n",
    "cond2 = col(\"origin\") == 'SFO'\n",
    "cond3 = col(\"destination\") == 'ORD'\n",
    "\n",
    "(\n",
    "    df.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    "    .where(cond1 & cond2 & cond3)\n",
    "    .orderBy(\"delay\", ascending=False)\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "environmental-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF For convert date to readable date\n",
    "def to_date_format_udf(d_str):\n",
    "    l = [char for char in d_str]\n",
    "    return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "postal-fight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/19  09:25'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_date_format_udf(\"02190925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "endangered-accountability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.to_date_format_udf(d_str)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Register the UDF\n",
    "spark.udf.register(\"to_date_format_udf\", to_date_format_udf,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "daily-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_format |\n",
      "+------------+\n",
      "|01/01  12:45|\n",
      "|01/02  06:00|\n",
      "|01/02  12:45|\n",
      "|01/02  06:05|\n",
      "|01/03  12:45|\n",
      "|01/03  06:05|\n",
      "|01/04  12:43|\n",
      "|01/04  06:05|\n",
      "|01/05  12:45|\n",
      "|01/05  06:05|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .selectExpr(\"to_date_format_udf(date) as data_format\")\n",
    "    .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "pleasant-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|    date|delay|distance|origin|destination|    date|     date_fm|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|01011245|    6|     602|   ABE|        ATL|01011245|01/01  12:45|\n",
      "|01020600|   -8|     369|   ABE|        DTW|01020600|01/02  06:00|\n",
      "|01021245|   -2|     602|   ABE|        ATL|01021245|01/02  12:45|\n",
      "|01020605|   -4|     602|   ABE|        ATL|01020605|01/02  06:05|\n",
      "|01031245|   -4|     602|   ABE|        ATL|01031245|01/03  12:45|\n",
      "|01030605|    0|     602|   ABE|        ATL|01030605|01/03  06:05|\n",
      "|01041243|   10|     602|   ABE|        ATL|01041243|01/04  12:43|\n",
      "|01040605|   28|     602|   ABE|        ATL|01040605|01/04  06:05|\n",
      "|01051245|   88|     602|   ABE|        ATL|01051245|01/05  12:45|\n",
      "|01050605|    9|     602|   ABE|        ATL|01050605|01/05  06:05|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT *, date, to_date_format_udf(date) as date_fm FROM\n",
    "us_delay_flights_tbl\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "exclusive-borough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+------+-----------+\n",
      "|to_date_format_udf(date)|delay|origin|destination|\n",
      "+------------------------+-----+------+-----------+\n",
      "|            02/19  09:25| 1638|   SFO|        ORD|\n",
      "|            01/03  17:55|  396|   SFO|        ORD|\n",
      "|            01/02  23:30|  326|   SFO|        ORD|\n",
      "|            01/05  12:05|  320|   SFO|        ORD|\n",
      "|            01/19  09:25|  297|   SFO|        ORD|\n",
      "|            02/17  11:15|  296|   SFO|        ORD|\n",
      "|            01/07  10:40|  279|   SFO|        ORD|\n",
      "|            01/05  15:50|  274|   SFO|        ORD|\n",
      "|            03/12  07:30|  266|   SFO|        ORD|\n",
      "|            01/26  11:04|  258|   SFO|        ORD|\n",
      "+------------------------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "SELECT to_date_format_udf(date), delay, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' and destination = 'ORD'\n",
    "ORDER BY delay DESC\n",
    "\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "driving-death",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\n",
    "FROM us_delay_flights_tbl\n",
    "ORDER BY origin, delay DESC\"\"\").show(10)\n",
    "\n",
    "# With API\n",
    "\n",
    "\n",
    "vld = col(\"delay\") > 360\n",
    "ld = (col(\"delay\") > 120) & (col(\"delay\") < 360)\n",
    "sh = (col(\"delay\") > 60) & (col(\"delay\") < 120)\n",
    "td = (col(\"delay\") > 0) & (col(\"delay\") < 60)\n",
    "nd = col(\"delay\") == 0\n",
    "\n",
    "(\n",
    "df.select(\"delay\",\"origin\", \"destination\"\n",
    "          ,when(vld,'Very Long Delays')\n",
    "          .when(nd,'No Delays')\n",
    "          .when(ld, 'Long Delays')\n",
    "          .when(sh,'Short Delays')\n",
    "          .when(td, 'Tolerable Delays')          \n",
    "          .when(nd,'No Delays')\n",
    "          .otherwise(\"Early\")\n",
    "          .alias(\"Flight_Delays\")\n",
    "         )\n",
    "    #.withColumnRenamed(\"origin\",\"no\")\n",
    "    .orderBy([\"origin\",\"delay\"],ascending=[1,0])\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "lasting-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 asc, 0 desc\n",
    "(\n",
    "    df.selectExpr(\"delay\", \"origin\", \"destination\",\"\"\"\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\"\"\")\n",
    "    .orderBy([\"origin\",\"delay\"],ascending=[1,0])\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "compound-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-active",
   "metadata": {},
   "source": [
    "### SQL Tables and Views\n",
    "#### Managed Versus UnmanagedTables\n",
    "##### 1. Creating Managed SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "owned-cruise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/home/usman/Documents/learningSpark/Chap4/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/home/usman/Documents/learningSpark/Chap4/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "recorded-alcohol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_origin_airport_jfk_tmp_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('learn_spark_db')\n",
    "#spark.sql(\"DROP TABLE us_origin_airport_jfk_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "polished-trunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "progressive-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP DATABASE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "recovered-anniversary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "impossible-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
    "\n",
    "\n",
    "    #You can do the same thing using the DataFrame API like this:\n",
    "# In Python\n",
    "# Path to our US flight delays CSV file\n",
    "    #csv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "# Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "sunset-language",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\n",
    "\n",
    "#spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "controversial-three",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(\"DROP TABLE managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "valid-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-membrane",
   "metadata": {},
   "source": [
    "##### 2. Creating an unmanaged table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "written-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "sized-warehouse",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table learn_spark_db.us_delay_flights_tbl already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-dc4e6f21d9a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m origin STRING, destination STRING)\n\u001b[1;32m      5\u001b[0m \u001b[0mUSING\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0mOPTIONS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m'/data/departuredelays.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \"\"\")\n\u001b[0m",
      "\u001b[0;32m~/Downloads/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learningSpark-Wj2k9VZ0/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table learn_spark_db.us_delay_flights_tbl already exists."
     ]
    }
   ],
   "source": [
    "#with Spark SQL\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE us_delay_flights_tbl (date STRING, delay INT, distance INT,\n",
    "origin STRING, destination STRING)\n",
    "USING csv OPTIONS (PATH '/data/departuredelays.csv')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "negative-skill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(\\n    df.write\\n    .option(\"path\", \"data/unmanaged/us_flights_delay\")\\n    .saveAsTable(\"us_delay_flights_tbl\")\\n)\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Will create folder and write unmanaged table\n",
    "# Spark API\n",
    "'''\n",
    "(\n",
    "    df.write\n",
    "    .option(\"path\", \"data/unmanaged/us_flights_delay\")\n",
    "    .saveAsTable(\"us_delay_flights_tbl\")\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "simplified-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP TABLE us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-nitrogen",
   "metadata": {},
   "source": [
    "###### 3. Creating Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "active-bosnia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with Spark SQL\n",
    "spark.sql(\n",
    "\n",
    "    #Origin = SFO\n",
    "    \n",
    "\"\"\"\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    "SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "origin = 'SFO';\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    \n",
    "    #Origin JFK\n",
    "    \n",
    "\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    "origin = 'JFK'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "angry-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select from view\n",
    "#spark.sql(\"SELECT date FROM us_origin_airport_SFO_global_tmp_view\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "round-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM managed_us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM managed_us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "minor-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02010900|   -1|   JFK|        LAX|\n",
      "|02011200|   -5|   JFK|        LAX|\n",
      "|02011030|   -6|   JFK|        LAX|\n",
      "|02011900|   -1|   JFK|        LAX|\n",
      "|02011700|   -3|   JFK|        LAS|\n",
      "|02010800|   -4|   JFK|        SFO|\n",
      "|02011540|    0|   JFK|        DFW|\n",
      "|02011705|   30|   JFK|        SAN|\n",
      "|02010800|   26|   JFK|        BOS|\n",
      "|02011530|   -2|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_jfk.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "minimal-consequence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_origin_airport_jfk_tmp_view', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "intended-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP TABLE us_origin_airport_jfk_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dirty-penny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010630|  -10|     928|   RSW|        EWR|\n",
      "|01021029|   87|     974|   RSW|        ORD|\n",
      "|01021346|    0|     928|   RSW|        EWR|\n",
      "|01021044|   18|     928|   RSW|        EWR|\n",
      "|01021730|   29|     748|   RSW|        IAH|\n",
      "|01020535|  605|     974|   RSW|        ORD|\n",
      "|01021820|   71|     974|   RSW|        ORD|\n",
      "|01021743|    0|     928|   RSW|        EWR|\n",
      "|01022017|    0|     928|   RSW|        EWR|\n",
      "|01020600|   -2|     748|   RSW|        IAH|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM managed_us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "heated-medicine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02010900|   -1|   JFK|        LAX|\n",
      "|02011200|   -5|   JFK|        LAX|\n",
      "|02011030|   -6|   JFK|        LAX|\n",
      "|02011900|   -1|   JFK|        LAX|\n",
      "|02011700|   -3|   JFK|        LAS|\n",
      "|02010800|   -4|   JFK|        SFO|\n",
      "|02011540|    0|   JFK|        DFW|\n",
      "|02011705|   30|   JFK|        SAN|\n",
      "|02010800|   26|   JFK|        BOS|\n",
      "|02011530|   -2|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "exact-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02010900|   -1|   JFK|        LAX|\n",
      "|02011200|   -5|   JFK|        LAX|\n",
      "|02011030|   -6|   JFK|        LAX|\n",
      "|02011900|   -1|   JFK|        LAX|\n",
      "|02011700|   -3|   JFK|        LAS|\n",
      "|02010800|   -4|   JFK|        SFO|\n",
      "|02011540|    0|   JFK|        DFW|\n",
      "|02011705|   30|   JFK|        SAN|\n",
      "|02010800|   26|   JFK|        BOS|\n",
      "|02011530|   -2|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-exploration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
