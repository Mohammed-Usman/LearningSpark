{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-creek",
   "metadata": {},
   "source": [
    "### Using Spark SQL in Spark Applications\n",
    "#### Using Spark SQL in Spark Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "precious-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "after-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "veterinary-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/departuredelays.csv\"\n",
    "\n",
    "file_schema = \"date STRING, delay INT, distance INT,origin STRING, destination STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "great-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By setting inferSchema=true, \n",
    "#Spark will automatically go through the csv file\n",
    "#and infer the schema of each column.\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(file_schema)\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .load(csv_file)\n",
    "    \n",
    ")\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "desperate-hands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nschema = \"`date` STRING, `delay` INT, `distance` INT,\\n`origin` STRING, `destination` STRING\"\\n\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you want to specify a schema, you can use a DDL-formatted\n",
    "#string. For example:\n",
    "\n",
    "# In Python\n",
    "'''\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT,\n",
    "`origin` STRING, `destination` STRING\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "occasional-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "honey-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "spark.sql('''\n",
    "SELECT distance, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE distance > 1000 ORDER BY distance DESC\n",
    "''').show(10)\n",
    "\n",
    "# With DateFrame API\n",
    "(\n",
    "    df.select('distance', 'origin', 'destination')\n",
    "    .where('distance > 1000')\n",
    "    .orderBy('distance', ascending=False)\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "planned-march",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "\n",
    "spark.sql('''\n",
    "\n",
    "SELECT date, delay, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' and destination = 'ORD'\n",
    "ORDER BY delay DESC\n",
    "''').show(10)\n",
    "\n",
    "# With API\n",
    "\n",
    "cond1 = col(\"delay\") > 120\n",
    "cond2 = col(\"origin\") == 'SFO'\n",
    "cond3 = col(\"destination\") == 'ORD'\n",
    "\n",
    "(\n",
    "    df.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    "    .where(cond1 & cond2 & cond3)\n",
    "    .orderBy(\"delay\", ascending=False)\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "environmental-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF For convert date to readable date\n",
    "def to_date_format_udf(d_str):\n",
    "    l = [char for char in d_str]\n",
    "    return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "postal-fight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/19  09:25'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_date_format_udf(\"02190925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "endangered-accountability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.to_date_format_udf(d_str)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Register the UDF\n",
    "spark.udf.register(\"to_date_format_udf\", to_date_format_udf,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daily-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_format |\n",
      "+------------+\n",
      "|01/01  12:45|\n",
      "|01/02  06:00|\n",
      "|01/02  12:45|\n",
      "|01/02  06:05|\n",
      "|01/03  12:45|\n",
      "|01/03  06:05|\n",
      "|01/04  12:43|\n",
      "|01/04  06:05|\n",
      "|01/05  12:45|\n",
      "|01/05  06:05|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .selectExpr(\"to_date_format_udf(date) as data_format\")\n",
    "    .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "pleasant-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|    date|delay|distance|origin|destination|    date|     date_fm|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|01011245|    6|     602|   ABE|        ATL|01011245|01/01  12:45|\n",
      "|01020600|   -8|     369|   ABE|        DTW|01020600|01/02  06:00|\n",
      "|01021245|   -2|     602|   ABE|        ATL|01021245|01/02  12:45|\n",
      "|01020605|   -4|     602|   ABE|        ATL|01020605|01/02  06:05|\n",
      "|01031245|   -4|     602|   ABE|        ATL|01031245|01/03  12:45|\n",
      "|01030605|    0|     602|   ABE|        ATL|01030605|01/03  06:05|\n",
      "|01041243|   10|     602|   ABE|        ATL|01041243|01/04  12:43|\n",
      "|01040605|   28|     602|   ABE|        ATL|01040605|01/04  06:05|\n",
      "|01051245|   88|     602|   ABE|        ATL|01051245|01/05  12:45|\n",
      "|01050605|    9|     602|   ABE|        ATL|01050605|01/05  06:05|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT *, date, to_date_format_udf(date) as date_fm FROM\n",
    "us_delay_flights_tbl\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "exclusive-borough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+------+-----------+\n",
      "|to_date_format_udf(date)|delay|origin|destination|\n",
      "+------------------------+-----+------+-----------+\n",
      "|            02/19  09:25| 1638|   SFO|        ORD|\n",
      "|            01/03  17:55|  396|   SFO|        ORD|\n",
      "|            01/02  23:30|  326|   SFO|        ORD|\n",
      "|            01/05  12:05|  320|   SFO|        ORD|\n",
      "|            01/19  09:25|  297|   SFO|        ORD|\n",
      "|            02/17  11:15|  296|   SFO|        ORD|\n",
      "|            01/07  10:40|  279|   SFO|        ORD|\n",
      "|            01/05  15:50|  274|   SFO|        ORD|\n",
      "|            03/12  07:30|  266|   SFO|        ORD|\n",
      "|            01/26  11:04|  258|   SFO|        ORD|\n",
      "+------------------------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "SELECT to_date_format_udf(date), delay, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' and destination = 'ORD'\n",
    "ORDER BY delay DESC\n",
    "\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "driving-death",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\n",
    "FROM us_delay_flights_tbl\n",
    "ORDER BY origin, delay DESC\"\"\").show(10)\n",
    "\n",
    "# With API\n",
    "\n",
    "\n",
    "vld = col(\"delay\") > 360\n",
    "ld = (col(\"delay\") > 120) & (col(\"delay\") < 360)\n",
    "sh = (col(\"delay\") > 60) & (col(\"delay\") < 120)\n",
    "td = (col(\"delay\") > 0) & (col(\"delay\") < 60)\n",
    "nd = col(\"delay\") == 0\n",
    "\n",
    "(\n",
    "df.select(\"delay\",\"origin\", \"destination\"\n",
    "          ,when(vld,'Very Long Delays')\n",
    "          .when(nd,'No Delays')\n",
    "          .when(ld, 'Long Delays')\n",
    "          .when(sh,'Short Delays')\n",
    "          .when(td, 'Tolerable Delays')          \n",
    "          .when(nd,'No Delays')\n",
    "          .otherwise(\"Early\")\n",
    "          .alias(\"Flight_Delays\")\n",
    "         )\n",
    "    #.withColumnRenamed(\"origin\",\"no\")\n",
    "    .orderBy([\"origin\",\"delay\"],ascending=[1,0])\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "lasting-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 asc, 0 desc\n",
    "(\n",
    "    df.selectExpr(\"delay\", \"origin\", \"destination\",\"\"\"\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\"\"\")\n",
    "    .orderBy([\"origin\",\"delay\"],ascending=[1,0])\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "compound-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-hands",
   "metadata": {},
   "source": [
    "### SQL Tables and Views\n",
    "#### Managed Versus UnmanagedTables\n",
    "##### Creating SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "micro-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"CREATE DATABASE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "structured-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP DATABASE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "physical-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abstract-grenada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You can do the same thing using the DataFrame API like this:\\n# In Python\\n# Path to our US flight delays CSV file\\n    csv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\\n# Schema as defined in the preceding example\\n    schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\\n    flights_df = spark.read.csv(csv_file, schema=schema)\\n    flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\\n\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
    "\n",
    "'''\n",
    "    You can do the same thing using the DataFrame API like this:\n",
    "# In Python\n",
    "# Path to our US flight delays CSV file\n",
    "    csv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "# Schema as defined in the preceding example\n",
    "    schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "    flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "    flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "injured-baghdad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "java.lang.RuntimeException: Error while running command to get file permissions : java.io.IOException: (null) entry in command string: null ls -F E:\\tmp\\hive\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:711)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:654)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:586)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:548)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:176)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:129)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:301)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:431)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:324)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:72)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:71)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:225)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:103)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:225)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:137)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:127)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:44)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:59)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:92)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:92)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:432)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:665)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:602)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-9cecaae71505>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"managed_us_delay_flights_tbl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\inventory\\.virtualenvs\\pysparkpractice-z24tukib\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\inventory\\.virtualenvs\\pysparkpractice-z24tukib\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\inventory\\.virtualenvs\\pysparkpractice-z24tukib\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\inventory\\.virtualenvs\\pysparkpractice-z24tukib\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: java.lang.RuntimeException: Error while running command to get file permissions : java.io.IOException: (null) entry in command string: null ls -F E:\\tmp\\hive\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:711)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:654)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:586)\r\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:548)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:176)\r\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:129)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:301)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:431)\r\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:324)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:72)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:71)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:225)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:103)\r\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:225)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:137)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:127)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:44)\r\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:59)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:92)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:92)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:432)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:665)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:602)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n;"
     ]
    }
   ],
   "source": [
    "df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-scanner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
