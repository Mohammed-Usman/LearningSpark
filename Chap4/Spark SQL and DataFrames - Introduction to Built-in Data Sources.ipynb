{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparable-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pipenv run pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-victor",
   "metadata": {},
   "source": [
    "### Using Spark SQL in Spark Applications\n",
    "#### Using Spark SQL in Spark Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precious-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "after-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "veterinary-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/departuredelays.csv\"\n",
    "\n",
    "file_schema = \"date STRING, delay INT, distance INT,origin STRING, destination STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "great-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By setting inferSchema=true, \n",
    "#Spark will automatically go through the csv file\n",
    "#and infer the schema of each column.\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(file_schema)\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .load(csv_file)\n",
    "    \n",
    ")\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "desperate-hands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nschema = \"`date` STRING, `delay` INT, `distance` INT,\\n`origin` STRING, `destination` STRING\"\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you want to specify a schema, you can use a DDL-formatted\n",
    "#string. For example:\n",
    "\n",
    "# In Python\n",
    "'''\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT,\n",
    "`origin` STRING, `destination` STRING\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occasional-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "honey-burke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "spark.sql('''\n",
    "SELECT distance, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE distance > 1000 ORDER BY distance DESC\n",
    "''').show(10)\n",
    "\n",
    "# With DateFrame API\n",
    "(\n",
    "    df.select('distance', 'origin', 'destination')\n",
    "    .where('distance > 1000')\n",
    "    .orderBy('distance', ascending=False)\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "planned-march",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925| 1638|   SFO|        ORD|\n",
      "|01031755|  396|   SFO|        ORD|\n",
      "|01022330|  326|   SFO|        ORD|\n",
      "|01051205|  320|   SFO|        ORD|\n",
      "|01190925|  297|   SFO|        ORD|\n",
      "|02171115|  296|   SFO|        ORD|\n",
      "|01071040|  279|   SFO|        ORD|\n",
      "|01051550|  274|   SFO|        ORD|\n",
      "|03120730|  266|   SFO|        ORD|\n",
      "|01261104|  258|   SFO|        ORD|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "\n",
    "spark.sql('''\n",
    "\n",
    "SELECT date, delay, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' and destination = 'ORD'\n",
    "ORDER BY delay DESC\n",
    "''').show(10)\n",
    "\n",
    "# With API\n",
    "\n",
    "cond1 = col(\"delay\") > 120\n",
    "cond2 = col(\"origin\") == 'SFO'\n",
    "cond3 = col(\"destination\") == 'ORD'\n",
    "\n",
    "(\n",
    "    df.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    "    .where(cond1 & cond2 & cond3)\n",
    "    .orderBy(\"delay\", ascending=False)\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "environmental-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF For convert date to readable date\n",
    "def to_date_format_udf(d_str):\n",
    "    l = [char for char in d_str]\n",
    "    return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "postal-fight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/19  09:25'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_date_format_udf(\"02190925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endangered-accountability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.to_date_format_udf(d_str)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Register the UDF\n",
    "spark.udf.register(\"to_date_format_udf\", to_date_format_udf,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daily-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_format |\n",
      "+------------+\n",
      "|01/01  12:45|\n",
      "|01/02  06:00|\n",
      "|01/02  12:45|\n",
      "|01/02  06:05|\n",
      "|01/03  12:45|\n",
      "|01/03  06:05|\n",
      "|01/04  12:43|\n",
      "|01/04  06:05|\n",
      "|01/05  12:45|\n",
      "|01/05  06:05|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .selectExpr(\"to_date_format_udf(date) as data_format\")\n",
    "    .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pleasant-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|    date|delay|distance|origin|destination|    date|     date_fm|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|01011245|    6|     602|   ABE|        ATL|01011245|01/01  12:45|\n",
      "|01020600|   -8|     369|   ABE|        DTW|01020600|01/02  06:00|\n",
      "|01021245|   -2|     602|   ABE|        ATL|01021245|01/02  12:45|\n",
      "|01020605|   -4|     602|   ABE|        ATL|01020605|01/02  06:05|\n",
      "|01031245|   -4|     602|   ABE|        ATL|01031245|01/03  12:45|\n",
      "|01030605|    0|     602|   ABE|        ATL|01030605|01/03  06:05|\n",
      "|01041243|   10|     602|   ABE|        ATL|01041243|01/04  12:43|\n",
      "|01040605|   28|     602|   ABE|        ATL|01040605|01/04  06:05|\n",
      "|01051245|   88|     602|   ABE|        ATL|01051245|01/05  12:45|\n",
      "|01050605|    9|     602|   ABE|        ATL|01050605|01/05  06:05|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT *, date, to_date_format_udf(date) as date_fm FROM\n",
    "us_delay_flights_tbl\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exclusive-borough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+------+-----------+\n",
      "|to_date_format_udf(date)|delay|origin|destination|\n",
      "+------------------------+-----+------+-----------+\n",
      "|            02/19  09:25| 1638|   SFO|        ORD|\n",
      "|            01/03  17:55|  396|   SFO|        ORD|\n",
      "|            01/02  23:30|  326|   SFO|        ORD|\n",
      "|            01/05  12:05|  320|   SFO|        ORD|\n",
      "|            01/19  09:25|  297|   SFO|        ORD|\n",
      "|            02/17  11:15|  296|   SFO|        ORD|\n",
      "|            01/07  10:40|  279|   SFO|        ORD|\n",
      "|            01/05  15:50|  274|   SFO|        ORD|\n",
      "|            03/12  07:30|  266|   SFO|        ORD|\n",
      "|            01/26  11:04|  258|   SFO|        ORD|\n",
      "+------------------------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "SELECT to_date_format_udf(date), delay, origin, destination FROM us_delay_flights_tbl\n",
    "WHERE delay > 120 AND origin = 'SFO' and destination = 'ORD'\n",
    "ORDER BY delay DESC\n",
    "\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "driving-death",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With Query\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\n",
    "FROM us_delay_flights_tbl\n",
    "ORDER BY origin, delay DESC\"\"\").show(10)\n",
    "\n",
    "# With API\n",
    "\n",
    "\n",
    "vld = col(\"delay\") > 360\n",
    "ld = (col(\"delay\") > 120) & (col(\"delay\") < 360)\n",
    "sh = (col(\"delay\") > 60) & (col(\"delay\") < 120)\n",
    "td = (col(\"delay\") > 0) & (col(\"delay\") < 60)\n",
    "nd = col(\"delay\") == 0\n",
    "\n",
    "(\n",
    "df.select(\"delay\",\"origin\", \"destination\"\n",
    "          ,when(vld,'Very Long Delays')\n",
    "          .when(nd,'No Delays')\n",
    "          .when(ld, 'Long Delays')\n",
    "          .when(sh,'Short Delays')\n",
    "          .when(td, 'Tolerable Delays')          \n",
    "          .when(nd,'No Delays')\n",
    "          .otherwise(\"Early\")\n",
    "          .alias(\"Flight_Delays\")\n",
    "         )\n",
    "    #.withColumnRenamed(\"origin\",\"no\")\n",
    "    .orderBy([\"origin\",\"delay\"],ascending=[1,0])\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lasting-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 asc, 0 desc\n",
    "(\n",
    "    df.selectExpr(\"delay\", \"origin\", \"destination\",\"\"\"\n",
    "CASE\n",
    "WHEN delay > 360 THEN 'Very Long Delays'\n",
    "WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    "WHEN delay = 0 THEN 'No Delays'\n",
    "ELSE 'Early'\n",
    "END AS Flight_Delays\"\"\")\n",
    "    .orderBy([\"origin\",\"delay\"],ascending=[1,0])\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "compound-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-active",
   "metadata": {},
   "source": [
    "### SQL Tables and Views\n",
    "#### Managed Versus UnmanagedTables\n",
    "##### Creating SQL Databases and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "polished-trunk",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Database learn_spark_db already exists;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9d52933d2eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE DATABASE learn_spark_db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/learningSpark-Wj2k9VZ0/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learningSpark-Wj2k9VZ0/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learningSpark-Wj2k9VZ0/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/learningSpark-Wj2k9VZ0/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Database learn_spark_db already exists;"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "progressive-tobacco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "recovered-anniversary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "impossible-forty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You can do the same thing using the DataFrame API like this:\\n# In Python\\n# Path to our US flight delays CSV file\\n    csv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\\n# Schema as defined in the preceding example\\n    schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\\n    flights_df = spark.read.csv(csv_file, schema=schema)\\n    flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
    "\n",
    "'''\n",
    "    You can do the same thing using the DataFrame API like this:\n",
    "# In Python\n",
    "# Path to our US flight delays CSV file\n",
    "    csv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "# Schema as defined in the preceding example\n",
    "    schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "    flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "    flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "endless-internet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "functional-honor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(\"DROP TABLE managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-boating",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
